{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations recorded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Functions- show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(training_results):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(training_results['training_loss'], 'r')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('training loss iterations')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(training_results['validation_accuracy'])\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1]))\n",
    "    plt.show()\n",
    "\n",
    "def show_dataComp(data_sample,y):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(data_sample[1].numpy().reshape(28, 28), cmap='gray')\n",
    "    plt.title('y = ' + str(y))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Img Pairing and Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) traindataComp-trainloader: consecutive data pairs from the training set <br>\n",
    "2) valdataComp-valloader: consecutive data pairs from the test set <br>\n",
    "3) testdata- testloader: all data pairs from test set- keeps track of the pair <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indlist(target):\n",
    "    indlist = [[],[],[],[],[]]\n",
    "    \n",
    "    for i in range(len(target)):\n",
    "        if target[i] < 5:\n",
    "            indlist[target[i]].append(i)\n",
    "    return indlist\n",
    "\n",
    "def subData(dataSet):\n",
    "    #gets all the indices of the data obsv with same y from the dataset that is passed in only for 0-4\n",
    "    indices = indlist(dataSet.targets)\n",
    "    # a list of datasets where each dataset has the data for the same number\n",
    "    subsets = []\n",
    "    [subsets.append(torch.utils.data.Subset(dataSet, i)) for i in indices]\n",
    "    return subsets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparisonDataConsecutive(dataSet):\n",
    "    subsets = subData(dataSet)\n",
    "    # a list of the size of the possible pairs\n",
    "    comp = []\n",
    "    for indi in range(len(subsets) - 1):\n",
    "        comp.append(int(min(len(subsets[indi]), len(subsets[indi + 1]))))\n",
    "    tot1 = sum(comp)\n",
    "    # img pair data stored\n",
    "    x = torch.zeros([tot1, 2, 28, 28], dtype=torch.float32)\n",
    "    # greater than or less than label stored\n",
    "    y = torch.zeros([tot1,1])\n",
    "    # 1 for first pic greater, 0 for first pic less\n",
    "    k = 0\n",
    "    # does the actual pairing\n",
    "    for i in range(len(subsets) - 1):\n",
    "        for j in range(int(comp[i] / 2)):\n",
    "            x[k][0] = subsets[i][j][0]\n",
    "            x[k][1] = subsets[i + 1][j][0]\n",
    "            y[k][0] = 0\n",
    "            k += 1\n",
    "        for j in range(int(comp[i] / 2), comp[i]):\n",
    "            x[k][1] = subsets[i][j][0]\n",
    "            x[k][0] = subsets[i + 1][j][0]\n",
    "            y[k][0] = 1\n",
    "            k += 1\n",
    "    return x,y/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparisonDataNonconsecutive(dataSet):\n",
    "    subsets = subData(dataSet)\n",
    "    # a dictionary of pairs and the size of the pairings\n",
    "    comp = {}\n",
    "    for i in range(len(subsets) - 2):\n",
    "        for j in range(i + 2, len(subsets), 1):\n",
    "            comp[(i, j)] = int(min(len(subsets[i]), len(subsets[j])))\n",
    "    tot = sum(comp.values())\n",
    "    # greater than or less than label stored\n",
    "    x = torch.zeros([tot, 2, 28, 28], dtype=torch.float32)\n",
    "    # 1 for first pic greater, 0 for first pic less\n",
    "    y = torch.zeros([tot, 1])\n",
    "    k = 0\n",
    "    #the pairing\n",
    "    for key, values in comp.items():\n",
    "        for value in range(int(values / 2)):\n",
    "            x[k][0] = subsets[key[0]][value][0]\n",
    "            x[k][1] = subsets[key[1]][value][0]\n",
    "            y[k][0] = 0\n",
    "            k += 1\n",
    "        for value in range(int(values / 2), values):\n",
    "            x[k][0] = subsets[key[1]][value][0]\n",
    "            x[k][1] = subsets[key[0]][value][0]\n",
    "            y[k][0] = 1\n",
    "            k += 1\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparisonAll(dataSet):\n",
    "    subsets = subData(dataSet)\n",
    "    # a dictionary of pairs(tuple) and the size of the pairings\n",
    "    comp = {}\n",
    "    for i in range(len(subsets) - 2):\n",
    "        for j in range(i + 1, len(subsets), 1):\n",
    "            comp[(i, j)] = int(min(len(subsets[i]), len(subsets[j])))\n",
    "    tot = sum(comp.values())\n",
    "    # greater than or less than label stored\n",
    "    x = torch.zeros([tot, 2, 28, 28], dtype=torch.float32)\n",
    "    # 1 for first pic greater, 0 for first pic less\n",
    "    y = torch.zeros([tot, 1])\n",
    "    z = list()\n",
    "    k = 0\n",
    "    # the pairining\n",
    "    for key, values in comp.items():\n",
    "        for value in range(int(values / 2)):\n",
    "            x[k][0] = subsets[key[0]][value][0]\n",
    "            x[k][1] = subsets[key[1]][value][0]\n",
    "            y[k][0] = 0\n",
    "            z.append(key)\n",
    "            k += 1\n",
    "        for value in range(int(values / 2), values):\n",
    "            x[k][0] = subsets[key[1]][value][0]\n",
    "            x[k][1] = subsets[key[0]][value][0]\n",
    "            y[k][0] = 1\n",
    "            a = key[0]\n",
    "            b = key[1]\n",
    "            c = (b,a)\n",
    "            z.append(c)\n",
    "            k += 1\n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainsetcomp(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.len = (x.shape[0])\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainsetcompwithA(Dataset):\n",
    "    def __init__(self, x, y, z):\n",
    "        self.len = (x.shape[0])\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "        #the pair\n",
    "        self.z = z\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index], self.z[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),])\n",
    "\n",
    "trainset = dsets.MNIST(root='./../data',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "valset = dsets.MNIST(root='./../data',\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = comparisonDataConsecutive(trainset)\n",
    "traindataComp = Trainsetcomp(x,y)\n",
    "x,y = comparisonDataConsecutive(valset)\n",
    "valdataComp = Trainsetcomp(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(traindataComp,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valdataComp,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True)\n",
    "x,y,z = comparisonAll(valset)\n",
    "testdata = TrainsetcompwithA(x,y,z)\n",
    "testloader = torch.utils.data.DataLoader(testdata,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=F)\n",
    "# in the test loader: z is a list of size 2 of two [64] shape tensors. - it turned the tuple into a list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "## showing what the z in the test loader is- ignore\n",
    "i = 1\n",
    "j = 0\n",
    "for x,y,z in testloader:\n",
    "    j+=1\n",
    "    p = x[i,:,:,:]\n",
    "    show_dataComp(p,y[i,:])\n",
    "    #print(z)\n",
    "    print(z[0][i],z[1][i])\n",
    "    if j >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 2 * 28 * 28\n",
    "hiddendim = [400,200,64]\n",
    "outd = 1\n",
    "# 0 if first image is less than and 1 if frist image is greater than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFull(nn.Module):\n",
    "    def __init__(self, ind,h1d,h2d,h3d,outd):\n",
    "        super(ModelFull, self).__init__()\n",
    "        self.lin1 = nn.Linear(ind, h1d)\n",
    "        self.lin2 = nn.Linear(h1d, h2d)\n",
    "        self.lin3 = nn.Linear(h2d, h3d)\n",
    "        self.lin4 = nn.Linear(h3d, outd)\n",
    "        self.activations = []\n",
    "        self.pairs = []\n",
    "\n",
    "    def forward(self, x, recActivations = False):\n",
    "        if recActivations:\n",
    "            x1 = torch.relu(self.lin1(x))\n",
    "            x2 = torch.relu(self.lin2(x1))\n",
    "            x3 = torch.relu(self.lin3(x2))\n",
    "            self.activations.append(x3)\n",
    "            x4 = torch.sigmoid(self.lin4(x3))\n",
    "            return x4\n",
    "        else:\n",
    "            x1 = torch.relu(self.lin1(x))\n",
    "            x2 = torch.relu(self.lin2(x1))\n",
    "            x3 = torch.relu(self.lin3(x2))\n",
    "            x4 = torch.sigmoid(self.lin4(x3))\n",
    "            return x4\n",
    "        \n",
    "    def setPair(self, pair):\n",
    "        self.pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, epochs = 30):\n",
    "    lossList = []\n",
    "    #time0 = time()\n",
    "    for i in range(epochs):\n",
    "        runningLoss = 0\n",
    "        for x, y in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(x.view(-1, 2 * 28 *28))\n",
    "            #print(yhat.shape)\n",
    "            #print(y.shape)\n",
    "            loss = criterion(yhat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            runningLoss += loss.item()\n",
    "        print('epoch ', i, ' loss: ', str(runningLoss / len(traindataComp)))\n",
    "        lossList.append(runningLoss / len(traindataComp))\n",
    "    return lossList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelFull(ind,hiddendim[0],hiddendim[1],hiddendim[2], outd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  loss:  0.008733639343116318\n",
      "epoch  1  loss:  0.004051011978181742\n",
      "epoch  2  loss:  0.002187853483837539\n",
      "epoch  3  loss:  0.0016020586082779382\n",
      "epoch  4  loss:  0.0011678018264181774\n",
      "epoch  5  loss:  0.0010088350091189793\n",
      "epoch  6  loss:  0.0008821851611956294\n",
      "epoch  7  loss:  0.0007930122579911089\n",
      "epoch  8  loss:  0.0006564705582842881\n",
      "epoch  9  loss:  0.0006028631571634687\n",
      "epoch  10  loss:  0.0014974820196506182\n",
      "epoch  11  loss:  0.0006955977505238364\n",
      "epoch  12  loss:  0.0004866103245524745\n",
      "epoch  13  loss:  0.00092757672381085\n",
      "epoch  14  loss:  0.000539636522171699\n",
      "epoch  15  loss:  0.00042297771180736115\n",
      "epoch  16  loss:  0.00031846098123743946\n",
      "epoch  17  loss:  0.00023021138210282025\n",
      "epoch  18  loss:  0.0002464844879236723\n",
      "epoch  19  loss:  0.00025379385122594275\n",
      "epoch  20  loss:  0.00013882958988714456\n",
      "epoch  21  loss:  0.00010032881407654586\n",
      "epoch  22  loss:  5.90960906863404e-05\n",
      "epoch  23  loss:  0.0007765317970140342\n",
      "epoch  24  loss:  0.001400851129518585\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)\n",
    "results = train(model,criterion,optimizer,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3802\n",
      "4004\n",
      "valset accuracy:  0.9495504495504495\n"
     ]
    }
   ],
   "source": [
    "## Consecutive test set\n",
    "totcount = 0\n",
    "correctcount = 0\n",
    "for x,y in valloader:\n",
    "    x = x.view(-1, 2 * 28 *28)\n",
    "    with torch.no_grad():\n",
    "        yhat = model(x)\n",
    "    #print(yhat.shape)\n",
    "    ones = torch.ones(yhat.shape)\n",
    "    yhat = torch.where(yhat>.9, ones, yhat)\n",
    "    z = torch.zeros(yhat.shape)\n",
    "    yhat = torch.where(yhat<0.1, z, yhat)\n",
    "    for i,j in zip(yhat,y):\n",
    "        if i[0] == j[0]:\n",
    "            correctcount+=1\n",
    "        totcount+=1\n",
    "print(correctcount)\n",
    "print(totcount)\n",
    "#print(len(dataPairing.valdataComp))\n",
    "print('valset accuracy: ', correctcount/totcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7714\n",
      "8008\n",
      "test set accuracy:  0.9632867132867133\n"
     ]
    }
   ],
   "source": [
    "## non consecutive test set\n",
    "for x,y in valloader:\n",
    "    x = x.view(-1, 2 * 28 *28)\n",
    "    with torch.no_grad():\n",
    "        yhat = model(x)\n",
    "    ones = torch.ones(yhat.shape)\n",
    "    yhat = torch.where(yhat>.5, ones, yhat)\n",
    "    z = torch.zeros(yhat.shape)\n",
    "    yhat = torch.where(yhat<0.5, z, yhat)\n",
    "    wrongC = 0\n",
    "    for i in range(len(y)):\n",
    "        #print(i)\n",
    "        if yhat[i][0] == y[i][0]:\n",
    "            correctcount+=1\n",
    "            #p = torch.reshape(x[i], (2,28,28))\n",
    "            #show_dataComp(p, y[i])\n",
    "            \n",
    "        else:\n",
    "            #if wrongC < 2:\n",
    "                #p = torch.reshape(x[i], (2,28,28))\n",
    "                #show_dataComp(p, yhat[i])\n",
    "            wrongC += 1\n",
    "        totcount+=1\n",
    "print(correctcount)\n",
    "print(totcount)\n",
    "print('test set accuracy: ', correctcount/totcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16126\n",
      "16944\n",
      "test set accuracy:  0.9517233238904627\n"
     ]
    }
   ],
   "source": [
    "## test data set where activations are recorded\n",
    "\n",
    "for x,y,z in testloader:\n",
    "    x = x.view(-1, 2 * 28 *28)\n",
    "    with torch.no_grad():\n",
    "        yhat = model(x,True)\n",
    "    model.setPair(z)\n",
    "    ones = torch.ones(yhat.shape)\n",
    "    yhat = torch.where(yhat>.5, ones, yhat)\n",
    "    z = torch.zeros(yhat.shape)\n",
    "    yhat = torch.where(yhat<0.5, z, yhat)\n",
    "    wrongC = 0\n",
    "    for i in range(len(y)):\n",
    "        #print(i)\n",
    "        if yhat[i][0] == y[i][0]:\n",
    "            correctcount+=1\n",
    "            #p = torch.reshape(x[i], (2,28,28))\n",
    "            #show_dataComp(p, y[i])\n",
    "            \n",
    "        else:\n",
    "            #if wrongC < 2:\n",
    "                #p = torch.reshape(x[i], (2,28,28))\n",
    "                #show_dataComp(p, yhat[i])\n",
    "            wrongC += 1\n",
    "        totcount+=1\n",
    "print(correctcount)\n",
    "print(totcount)\n",
    "print('test set accuracy: ', correctcount/totcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "torch.Size([64, 64])\n",
      "140\n",
      "torch.Size([64])\n",
      "tensor([[0.3978, 0.2578, 0.8092,  ..., 0.3629, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8148, 2.9453,  ..., 1.5833, 0.0374, 0.0000],\n",
      "        [0.0000, 0.5649, 2.2676,  ..., 1.2280, 0.0095, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.5782, 1.9103,  ..., 0.8929, 0.0000, 0.0000],\n",
      "        [1.5552, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1036, 0.3663, 1.1732,  ..., 0.5707, 0.0036, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "len(testdata)\n",
    "print(len(testloader))\n",
    "print(model.activations[0].shape)\n",
    "print(len(model.pairs))\n",
    "print(model.pairs[0][1].shape)\n",
    "print(model.activations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSA preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# incomplete\n",
    "for pair,activ in zip(model.pairs, model.activations):\n",
    "    for i in range(len(pair[0])):\n",
    "        p1= pair[0][i].item()\n",
    "        p2 = pair[1][i].item()\n",
    "        t = (p1,p2)\n",
    "        print(t)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytochJN",
   "language": "python",
   "name": "pytochjn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
